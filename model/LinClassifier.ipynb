{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986266a1-d506-4c2c-a09e-29a9deff86a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df4b5588-3c25-477a-8b0d-38e785e05bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7626c1be-4b33-4e56-917a-306b9e7678f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "\n",
    "initialCSV = pd.read_csv('example.csv')\n",
    "\n",
    "# Partition the data into 80% training data and 20% testing data\n",
    "\n",
    "num_rows = len(initialCSV.index)\n",
    "first80p = math.floor(0.8 * num_rows)\n",
    "last20p = num_rows - first80p\n",
    "\n",
    "x_train = initialCSV.head(first80p).drop(columns = ['col3'])\n",
    "y_train = initialCSV.head(first80p)[['col3']]\n",
    "\n",
    "x_test = initialCSV.tail(last20p).drop(columns = ['col3'])\n",
    "y_test = initialCSV.tail(last20p)[['col3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b80ac-6122-47b0-8275-e71c0da3b70d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate avg. (mu) and stdev. (sigma) using the training set\n",
    "\n",
    "d = x_train.shape[1]\n",
    "mu = np.mean(x_train, axis = 0).values.reshape(1, d)\n",
    "sigma = np.std(x_train, axis = 0).values.reshape(1, d)\n",
    "\n",
    "# Transform the training features \n",
    "\n",
    "x_train = (x_train - mu) / (sigma + 1E-6)\n",
    "\n",
    "# Transform the testing features\n",
    "\n",
    "x_test = (x_test - mu) / (sigma + 1E-6)\n",
    "\n",
    "print('Test Mean = ')\n",
    "print(np.mean(x_test, axis = 0))\n",
    "\n",
    "print('Test Standard Deviation = ')\n",
    "print(np.std(x_test, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44a135-658b-4b2a-aeb0-c9d52ba92e26",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The objective function is $Q(w; X, y) = \\frac{1}{n} \\sum_{i = 1}^n \\log\n",
    "\\Big(1 + \\exp \\big(-y_i \\; x_i^T \\; w \\big) \\Big) + \\frac{\\lambda}{2} \\|w\\|_2^2$\n",
    "\n",
    "When $\\lambda = 0\\;$, the model is typical logistic regression. When $\\lambda > 0\\;$, the model becomes regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "204d1e0b-3af3-42de-b9ce-e5f9b0e0ff29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the value of the objective function (i.e. loss)\n",
    "\n",
    "# Inputs: \n",
    "#          weight (w)   -->  d x 1 matrix\n",
    "#          data (x)     -->  n x d matrix\n",
    "#          label (y)    -->  n x 1 matrix\n",
    "#          lmd (scalar) -->  regularization parameter\n",
    "\n",
    "# Output: loss as a scalar\n",
    "\n",
    "def objective(w, x , y, lmd):\n",
    "    \n",
    "    summ = 0.0\n",
    "    n = len(x.index)\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        \n",
    "        xi = x.iloc[[n - 1]]\n",
    "        yi = y.iloc[[n - 1]]\n",
    "        \n",
    "        expo = np.exp(-1 * np.dot(yi, np.dot(xi, w))[0][0])\n",
    "        summ = summ + math.log(1 + expo, 10)\n",
    "    \n",
    "    return (summ / n) + 0.5 * lmd * np.square(w).sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840ba44-849f-4aa0-a046-c707181174c7",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "The gradient at $w$ for regularized logistic regression is $g = - \\frac{1}{n} \\sum_{i = 1}^n \\frac{y_i\\;x_i}{1\\;+\\;\\exp (\\;y_i\\;x_i^T\\;w\\;)} + \\lambda\\;w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ec78ac7-54ef-4a0a-a14f-1558407432f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the gradient at w\n",
    "\n",
    "# Inputs: \n",
    "#          weight (w)   -->  d x 1 matrix\n",
    "#          data (x)     -->  n x d matrix\n",
    "#          label (y)    -->  n x 1 matrix\n",
    "#          lmd (scalar) -->  regularization parameter\n",
    "\n",
    "# Output: gradient (g) as a d x 1 matrix\n",
    "\n",
    "def gradient(w, x, y, lmd):\n",
    "    \n",
    "    summ = 0.0\n",
    "    n = len(x.index)\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        \n",
    "        # We want row i as a column\n",
    "        xi = x.iloc[[n - 1]].T \n",
    "        yi = y.iloc[[n - 1]]\n",
    "        \n",
    "        expo = np.exp(np.dot(yi, np.dot(xi.T, w))[0][0])\n",
    "        summ = summ + np.dot(xi, yi) / (1 + expo)\n",
    "        \n",
    "    return (lmd * w) - (summ / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0812c68-412b-4ab3-ad2c-5d71b1b93273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gradient Descent for logistic regression\n",
    "# Optimal weights will be obtained iteratively \n",
    "\n",
    "# Inputs: \n",
    "#          data (x)      -->  n x d matrix\n",
    "#          label (y)     -->  n x 1 matrix\n",
    "#          lmd (scalar)  -->  regularization parameter\n",
    "#          learning_rate -->  scalar\n",
    "#          weights (w)   -->  d x 1 matrix (initial)\n",
    "#          max_epochs    -->  integer\n",
    "\n",
    "# Outputs: \n",
    "#          weights (w)   -->  d x 1 matrix (final) \n",
    "#          objvals       -->  a record of each epoch's objective value \n",
    "\n",
    "def gradient_descent(x, y, lmd, learning_rate, w, max_epochs = 100):\n",
    "    \n",
    "    objvals = []\n",
    "    \n",
    "    for i in range(max_epochs):\n",
    "        \n",
    "        gt = gradient(w, x, y, lmd)\n",
    "        w = w - learning_rate * gt\n",
    "        objvals = objvals + [objective(w, x, y, lmd).iloc[0]]\n",
    "    \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d8115-567e-4f35-a9c5-187993c0949d",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Use Gradient Descent to obtain optimal weights and a list of objective values for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9f957ed2-41c6-434b-9fa7-740653d6add4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# Assumes that the number of data columns is 2\n",
    "\n",
    "weights = np.random.randn(2, 1)\n",
    "weights = pd.DataFrame(weights, columns = ['weights'])\n",
    "\n",
    "logreg = gradient_descent(x_train, y_train, 0, 0.1, weights, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3caa7b95-176a-40a2-9158-7f2a7db8ce06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Regularized Logistic Regression\n",
    "# Assumes that the number of data columns is 2\n",
    "\n",
    "weightsR = np.random.randn(2, 1)\n",
    "weightsR = pd.DataFrame(weightsR, columns = ['weights'])\n",
    "\n",
    "reglogreg = gradient_descent(x_train, y_train, 0.5, 0.1, weightsR, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb3a7f-7029-4f95-97f3-c1cc138bc288",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c0a8881-7c79-4c8e-b7ea-7772b0e748cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict the class label\n",
    "\n",
    "# Inputs: \n",
    "#          weights (w)  -->  d x 1 matrix\n",
    "#          data (X)     -->  m x d matrix\n",
    "\n",
    "# Output: predictions (f) as an m x 1 matrix\n",
    "\n",
    "def predict(w, X):\n",
    "    return np.dot(X, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80e70a-e70f-47bc-8e1b-036f6f6ba764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the training error\n",
    "\n",
    "train_pred_GD_N = predict(logreg[0], x_train)\n",
    "train_pred_GD_R = predict(reglogreg[0], x_train)\n",
    "\n",
    "# [0] may need to be appended to these 2 lines when not indexing a scalar\n",
    "train_MSE_GD_N = np.mean((train_pred_GD_N - y_train) ** 2)\n",
    "train_MSE_GD_R = np.mean((train_pred_GD_R - y_train) ** 2)\n",
    "\n",
    "print(train_MSE_GD_N, '\\t', train_MSE_GD_R) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feeec93-ebf1-4e59-909d-d77da038c960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the testing error\n",
    "\n",
    "test_pred_GD_N = predict(logreg[0], x_test)\n",
    "test_pred_GD_R = predict(reglogreg[0], x_test)\n",
    "\n",
    "# [0] may need to be appended to these 2 lines when not indexing a scalar\n",
    "test_MSE_GD_N = np.mean((test_pred_GD_N - y_test) ** 2)\n",
    "test_MSE_GD_R = np.mean((test_pred_GD_R - y_test) ** 2)\n",
    "\n",
    "print(test_MSE_GD_N, '\\t', test_MSE_GD_R) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
